# Expectation-Maximization (EM) Algorithm

## 1. Concept Skeleton
**Definition:** Iterative method for maximum likelihood with latent variables; alternates E-step (compute expected log-likelihood) and M-step (maximize); monotonically increases likelihood  
**Purpose:** Handle missing data, mixture models, hidden Markov models; simplify complex likelihood; guaranteed convergence to local maximum  
**Prerequisites:** Maximum likelihood, latent variables, Jensen's inequality, conditional expectation, complete-data likelihood

## 2. Comparative Framing
| Method | EM Algorithm | Direct MLE | Gibbs Sampling | Variational Bayes | Gradient Ascent | Data Augmentation |
|--------|--------------|------------|----------------|-------------------|-----------------|-------------------|
| **Latent Variables** | Yes (E-step integrates) | No (marginalized) | Yes (sampled) | Yes (approximated) | No | Yes (sampled) |
| **Convergence** | Monotonic (guaranteed) | Depends on optimizer | Asymptotic | Lower bound | Depends on step size | Asymptotic |
| **Speed** | Moderate | Fast (if tractable) | Slow | Fast | Fast (with good gradient) | Moderate |
| **Global Optimum** | No (local) | No | Explores modes | No | No | Explores modes |
| **Closed Form** | Often M-step | Rarely | N/A | Sometimes | N/A | Conditional |
| **Missing Data** | Natural framework | Requires integration | Imputes | Imputes | Difficult | Imputes |

## 3. Examples + Counterexamples

**Classic Example:**  
Gaussian mixture model (K=2 components): EM separates clusters with soft assignments. E-step computes responsibilities Î³áµ¢â‚–=P(cluster k|xáµ¢). M-step updates Î¼â‚–, Î£â‚–, Ï€â‚–. Converges in 20-50 iterations with log-likelihood increase ~0.1% per iteration.

**Failure Case:**  
Identifiability issue: Mixture labels arbitrary (label switching). Multiple local maxima depending on initialization. Solution: Run multiple random starts, select highest likelihood.

**Edge Case:**  
Censored regression (Tobit): Latent Y*áµ¢ unobserved when Yáµ¢=0. E-step computes E[Y*áµ¢|Yáµ¢=0, Î¸]. M-step OLS on completed data. Equivalent to direct MLE but conceptually clearer.

## 4. Layer Breakdown
```
EM Algorithm Framework:
â”œâ”€ Problem Setup:
â”‚   â”œâ”€ Observed Data: Y = {yâ‚, ..., yâ‚™}
â”‚   â”œâ”€ Latent Variables: Z = {zâ‚, ..., zâ‚™} (unobserved)
â”‚   â”œâ”€ Complete Data: X = (Y, Z)
â”‚   â”œâ”€ Complete-Data Likelihood: L_c(Î¸|X) = p(Y, Z|Î¸)
â”‚   â”œâ”€ Observed-Data Likelihood: L(Î¸|Y) = âˆ« p(Y, Z|Î¸) dZ
â”‚   â””â”€ Goal: Maximize L(Î¸|Y) when direct optimization intractable
â”œâ”€ E-Step (Expectation):
â”‚   â”œâ”€ Compute Q-function:
â”‚   â”‚   â”œâ”€ Q(Î¸|Î¸â½áµ—â¾) = E_Z|Y,Î¸â½áµ—â¾[log L_c(Î¸|Y,Z)]
â”‚   â”‚   â”œâ”€ Expectation over posterior p(Z|Y, Î¸â½áµ—â¾)
â”‚   â”‚   â””â”€ Use current parameter Î¸â½áµ—â¾
â”‚   â”œâ”€ Sufficient Statistics:
â”‚   â”‚   â”œâ”€ Compute E[T(Z)|Y, Î¸â½áµ—â¾] where T(Z) are sufficient statistics
â”‚   â”‚   â””â”€ Often simpler than full conditional distribution
â”‚   â”œâ”€ Responsibilities (Mixture Models):
â”‚   â”‚   â”œâ”€ Î³áµ¢â‚– = P(Záµ¢=k|Yáµ¢, Î¸â½áµ—â¾)
â”‚   â”‚   â”œâ”€ Bayes rule: Î³áµ¢â‚– = Ï€â‚–Â·f(yáµ¢|Î¸â‚–) / Î£â±¼ Ï€â±¼Â·f(yáµ¢|Î¸â±¼)
â”‚   â”‚   â””â”€ Soft cluster assignments
â”‚   â”œâ”€ Conditional Expectations:
â”‚   â”‚   â”œâ”€ Censored data: E[Yáµ¢*|Yáµ¢ censored, Î¸â½áµ—â¾]
â”‚   â”‚   â”œâ”€ Missing data: E[Yáµ¢_missing|Yáµ¢_observed, Î¸â½áµ—â¾]
â”‚   â”‚   â””â”€ Latent factors: E[fáµ¢|Yáµ¢, Î¸â½áµ—â¾]
â”‚   â””â”€ Computational Methods:
â”‚       â”œâ”€ Analytical (Gaussian models)
â”‚       â”œâ”€ Monte Carlo EM (intractable E-step)
â”‚       â””â”€ Stochastic EM (sample Z instead of integrate)
â”œâ”€ M-Step (Maximization):
â”‚   â”œâ”€ Definition:
â”‚   â”‚   â”œâ”€ Î¸â½áµ—âºÂ¹â¾ = argmax_Î¸ Q(Î¸|Î¸â½áµ—â¾)
â”‚   â”‚   â”œâ”€ Maximize expected complete-data log-likelihood
â”‚   â”‚   â””â”€ Often has closed-form solution
â”‚   â”œâ”€ Parameter Updates:
â”‚   â”‚   â”œâ”€ Use sufficient statistics from E-step
â”‚   â”‚   â”œâ”€ Often weighted MLE (weights = responsibilities)
â”‚   â”‚   â””â”€ Example (mixture): Î¼â‚– = Î£áµ¢ Î³áµ¢â‚–Â·yáµ¢ / Î£áµ¢ Î³áµ¢â‚–
â”‚   â”œâ”€ Constraints:
â”‚   â”‚   â”œâ”€ Mixing proportions: Î£â‚– Ï€â‚– = 1
â”‚   â”‚   â”œâ”€ Covariance positive definite
â”‚   â”‚   â””â”€ Lagrange multipliers or constrained optimization
â”‚   â””â”€ Generalized M-step (GEM):
â”‚       â”œâ”€ Only require Q(Î¸â½áµ—âºÂ¹â¾) â‰¥ Q(Î¸â½áµ—â¾)
â”‚       â””â”€ Useful when maximization intractable
â”œâ”€ Convergence Properties:
â”‚   â”œâ”€ Monotonic Increase:
â”‚   â”‚   â”œâ”€ L(Î¸â½áµ—âºÂ¹â¾|Y) â‰¥ L(Î¸â½áµ—â¾|Y) (guaranteed)
â”‚   â”‚   â”œâ”€ Proof via Jensen's inequality
â”‚   â”‚   â””â”€ Likelihood never decreases
â”‚   â”œâ”€ Convergence to Local Maximum:
â”‚   â”‚   â”œâ”€ lim_{tâ†’âˆ} Î¸â½áµ—â¾ = Î¸* where âˆ‡L(Î¸*) = 0
â”‚   â”‚   â”œâ”€ Not necessarily global maximum
â”‚   â”‚   â””â”€ Depends on initialization
â”‚   â”œâ”€ Convergence Rate:
â”‚   â”‚   â”œâ”€ Linear convergence: ||Î¸â½áµ—âºÂ¹â¾ - Î¸*|| â‰ˆ Î»Â·||Î¸â½áµ—â¾ - Î¸*||
â”‚   â”‚   â”œâ”€ Î» = fraction of missing information
â”‚   â”‚   â””â”€ Slower with more missing data
â”‚   â”œâ”€ Stopping Criteria:
â”‚   â”‚   â”œâ”€ Parameter change: ||Î¸â½áµ—âºÂ¹â¾ - Î¸â½áµ—â¾|| < Îµ
â”‚   â”‚   â”œâ”€ Likelihood change: |Lâ½áµ—âºÂ¹â¾ - Lâ½áµ—â¾| < Îµ
â”‚   â”‚   â”œâ”€ Relative change: |Lâ½áµ—âºÂ¹â¾ - Lâ½áµ—â¾|/|Lâ½áµ—â¾| < Îµ
â”‚   â”‚   â””â”€ Maximum iterations: t > t_max
â”‚   â””â”€ Aitken Acceleration:
â”‚       â”œâ”€ Estimate Î¸* from sequence {Î¸â½áµ—â¾}
â”‚       â””â”€ Î¸Ìƒ = Î¸â½áµ—â¾ + Î”Î¸â½áµ—â¾/(1 - Î»Ì‚)
â”œâ”€ Theoretical Justification:
â”‚   â”œâ”€ Jensen's Inequality:
â”‚   â”‚   â”œâ”€ log L(Î¸|Y) = log âˆ« p(Y,Z|Î¸) dZ
â”‚   â”‚   â”œâ”€ = log âˆ« [p(Y,Z|Î¸)/q(Z)]Â·q(Z) dZ
â”‚   â”‚   â”œâ”€ â‰¥ âˆ« q(Z)Â·log[p(Y,Z|Î¸)/q(Z)] dZ (Jensen)
â”‚   â”‚   â””â”€ = ELBO (evidence lower bound)
â”‚   â”œâ”€ Variational Lower Bound:
â”‚   â”‚   â”œâ”€ log L(Î¸) â‰¥ E_q[log p(Y,Z|Î¸)] + H(q)
â”‚   â”‚   â”œâ”€ E-step: Set q(Z) = p(Z|Y,Î¸â½áµ—â¾) (tighten bound)
â”‚   â”‚   â””â”€ M-step: Maximize bound w.r.t. Î¸
â”‚   â”œâ”€ KL Divergence Decomposition:
â”‚   â”‚   â”œâ”€ log L(Î¸) = ELBO + KL(q||p(Z|Y,Î¸))
â”‚   â”‚   â”œâ”€ E-step sets KL = 0
â”‚   â”‚   â””â”€ M-step increases ELBO
â”‚   â””â”€ Why Monotonic:
â”‚       â”œâ”€ L(Î¸â½áµ—âºÂ¹â¾) â‰¥ ELBO(Î¸â½áµ—âºÂ¹â¾, qâ½áµ—â¾) (definition)
â”‚       â”œâ”€ â‰¥ ELBO(Î¸â½áµ—â¾, qâ½áµ—â¾) (M-step increases)
â”‚       â””â”€ = L(Î¸â½áµ—â¾) (E-step tightens)
â”œâ”€ Gaussian Mixture Model (GMM):
â”‚   â”œâ”€ Model:
â”‚   â”‚   â”œâ”€ p(yáµ¢) = Î£â‚– Ï€â‚–Â·ğ’©(yáµ¢|Î¼â‚–, Î£â‚–)
â”‚   â”‚   â”œâ”€ Ï€â‚–: Mixing proportions (Î£â‚– Ï€â‚– = 1)
â”‚   â”‚   â”œâ”€ K components
â”‚   â”‚   â””â”€ Latent: Záµ¢ âˆˆ {1,...,K} cluster membership
â”‚   â”œâ”€ E-Step:
â”‚   â”‚   â”œâ”€ Responsibilities: Î³áµ¢â‚– = Ï€â‚–Â·ğ’©(yáµ¢|Î¼â‚–,Î£â‚–) / Î£â±¼ Ï€â±¼Â·ğ’©(yáµ¢|Î¼â±¼,Î£â±¼)
â”‚   â”‚   â”œâ”€ P(Záµ¢=k|yáµ¢, Î¸â½áµ—â¾)
â”‚   â”‚   â””â”€ Soft assignments (sum to 1)
â”‚   â”œâ”€ M-Step:
â”‚   â”‚   â”œâ”€ nâ‚– = Î£áµ¢ Î³áµ¢â‚– (effective sample size)
â”‚   â”‚   â”œâ”€ Ï€â‚– = nâ‚– / n
â”‚   â”‚   â”œâ”€ Î¼â‚– = Î£áµ¢ Î³áµ¢â‚–Â·yáµ¢ / nâ‚– (weighted mean)
â”‚   â”‚   â””â”€ Î£â‚– = Î£áµ¢ Î³áµ¢â‚–Â·(yáµ¢-Î¼â‚–)(yáµ¢-Î¼â‚–)' / nâ‚– (weighted cov)
â”‚   â”œâ”€ Initialization:
â”‚   â”‚   â”œâ”€ K-means clustering
â”‚   â”‚   â”œâ”€ Random assignment
â”‚   â”‚   â””â”€ Multiple random starts
â”‚   â””â”€ Identifiability:
â”‚       â”œâ”€ Label switching (permutation invariance)
â”‚       â””â”€ Post-hoc label alignment
â”œâ”€ Missing Data:
â”‚   â”œâ”€ Missing at Random (MAR):
â”‚   â”‚   â”œâ”€ P(missing|Y_obs, Y_miss) = P(missing|Y_obs)
â”‚   â”‚   â”œâ”€ EM valid under MAR
â”‚   â”‚   â””â”€ Ignorable missingness mechanism
â”‚   â”œâ”€ Missing Completely at Random (MCAR):
â”‚   â”‚   â”œâ”€ P(missing) constant
â”‚   â”‚   â””â”€ Stronger assumption
â”‚   â”œâ”€ Not Missing at Random (NMAR):
â”‚   â”‚   â”œâ”€ P(missing|Y_obs, Y_miss) depends on Y_miss
â”‚   â”‚   â”œâ”€ EM biased
â”‚   â”‚   â””â”€ Need selection model
â”‚   â”œâ”€ E-Step:
â”‚   â”‚   â”œâ”€ Impute missing values: E[Y_miss|Y_obs, Î¸â½áµ—â¾]
â”‚   â”‚   â”œâ”€ Predict from observed data
â”‚   â”‚   â””â”€ Account for uncertainty
â”‚   â””â”€ M-Step:
â”‚       â”œâ”€ MLE using observed + imputed data
â”‚       â””â”€ Standard complete-data estimators
â”œâ”€ Censored/Truncated Data:
â”‚   â”œâ”€ Tobit Model (Censoring):
â”‚   â”‚   â”œâ”€ Latent: Yáµ¢* = Xáµ¢Î² + Îµáµ¢
â”‚   â”‚   â”œâ”€ Observed: Yáµ¢ = max(0, Yáµ¢*)
â”‚   â”‚   â”œâ”€ E-step: E[Yáµ¢*|Yáµ¢=0, Î¸â½áµ—â¾] = Xáµ¢Î² - ÏƒÂ·Î» where Î»=Ï†/Î¦ (IMR)
â”‚   â”‚   â””â”€ M-step: OLS on completed data
â”‚   â”œâ”€ Truncation:
â”‚   â”‚   â”œâ”€ Observations only if Y > c
â”‚   â”‚   â””â”€ Conditional distribution p(Y|Y>c, Î¸)
â”‚   â””â”€ Interval Censoring:
â”‚       â”œâ”€ Y âˆˆ [L, U]
â”‚       â””â”€ E-step: E[Y|L<Y<U, Î¸â½áµ—â¾]
â”œâ”€ Hidden Markov Models (HMM):
â”‚   â”œâ”€ Model:
â”‚   â”‚   â”œâ”€ States: Sâ‚œ âˆˆ {1,...,K} (latent Markov chain)
â”‚   â”‚   â”œâ”€ Observations: Yâ‚œ|Sâ‚œ ~ f(Â·|Î¸_Sâ‚œ)
â”‚   â”‚   â”œâ”€ Transition: P(Sâ‚œ=j|Sâ‚œâ‚‹â‚=i) = Aáµ¢â±¼
â”‚   â”‚   â””â”€ Emission: P(Yâ‚œ|Sâ‚œ=k) = fâ‚–(yâ‚œ)
â”‚   â”œâ”€ Forward-Backward Algorithm (E-Step):
â”‚   â”‚   â”œâ”€ Forward: Î±â‚œ(k) = P(Yâ‚:â‚œ, Sâ‚œ=k)
â”‚   â”‚   â”œâ”€ Backward: Î²â‚œ(k) = P(Yâ‚œâ‚Šâ‚:â‚œ|Sâ‚œ=k)
â”‚   â”‚   â”œâ”€ Smoothing: Î³â‚œ(k) = P(Sâ‚œ=k|Yâ‚:â‚œ) âˆ Î±â‚œ(k)Â·Î²â‚œ(k)
â”‚   â”‚   â””â”€ Pairwise: Î¾â‚œ(i,j) = P(Sâ‚œ=i, Sâ‚œâ‚Šâ‚=j|Yâ‚:â‚œ)
â”‚   â”œâ”€ M-Step:
â”‚   â”‚   â”œâ”€ Initial: Ï€â‚€(k) = Î³â‚(k)
â”‚   â”‚   â”œâ”€ Transition: Aáµ¢â±¼ = Î£â‚œ Î¾â‚œ(i,j) / Î£â‚œ Î³â‚œ(i)
â”‚   â”‚   â””â”€ Emission: Update Î¸â‚– using {yâ‚œ} weighted by Î³â‚œ(k)
â”‚   â””â”€ Applications:
â”‚       â”œâ”€ Speech recognition
â”‚       â”œâ”€ Regime-switching models (finance)
â”‚       â””â”€ Biological sequences
â”œâ”€ Factor Analysis:
â”‚   â”œâ”€ Model:
â”‚   â”‚   â”œâ”€ Yáµ¢ = Î›Â·fáµ¢ + Îµáµ¢
â”‚   â”‚   â”œâ”€ fáµ¢ ~ ğ’©(0, I) latent factors
â”‚   â”‚   â”œâ”€ Îµáµ¢ ~ ğ’©(0, Î¨) unique variances (diagonal)
â”‚   â”‚   â””â”€ Yáµ¢ ~ ğ’©(0, Î›Î›' + Î¨)
â”‚   â”œâ”€ E-Step:
â”‚   â”‚   â”œâ”€ E[fáµ¢|Yáµ¢, Î¸â½áµ—â¾] = (Î›'Î¨â»Â¹Î› + I)â»Â¹Î›'Î¨â»Â¹Yáµ¢
â”‚   â”‚   â””â”€ E[fáµ¢fáµ¢'|Yáµ¢, Î¸â½áµ—â¾] = Var(fáµ¢|Yáµ¢) + E[fáµ¢|Yáµ¢]E[fáµ¢|Yáµ¢]'
â”‚   â”œâ”€ M-Step:
â”‚   â”‚   â”œâ”€ Î› = [Î£áµ¢ Yáµ¢ E[fáµ¢]'][Î£áµ¢ E[fáµ¢fáµ¢']]â»Â¹
â”‚   â”‚   â””â”€ Î¨ = diag{(1/n)Î£áµ¢ Yáµ¢Yáµ¢' - Î› E[fáµ¢Yáµ¢']}
â”‚   â””â”€ Rotation Indeterminacy:
â”‚       â””â”€ Post-hoc rotation (varimax, etc.)
â”œâ”€ Variants & Extensions:
â”‚   â”œâ”€ Stochastic EM (SEM):
â”‚   â”‚   â”œâ”€ E-step: Sample Z ~ p(Z|Y, Î¸â½áµ—â¾) instead of integrate
â”‚   â”‚   â”œâ”€ M-step: Maximize using sampled Z
â”‚   â”‚   â””â”€ Better exploration of parameter space
â”‚   â”œâ”€ Monte Carlo EM (MCEM):
â”‚   â”‚   â”œâ”€ E-step intractable: Use MC integration
â”‚   â”‚   â”œâ”€ QÌ‚(Î¸) = (1/M)Î£â‚˜ log p(Y, Zâ½áµâ¾|Î¸)
â”‚   â”‚   â””â”€ Increase M as iterations progress
â”‚   â”œâ”€ Expectation-Conditional Maximization (ECM):
â”‚   â”‚   â”œâ”€ M-step in blocks (easier optimization)
â”‚   â”‚   â”œâ”€ CM-step 1: Maximize Î¸â‚ given Î¸â‚‚â½áµ—â¾
â”‚   â”‚   â”œâ”€ CM-step 2: Maximize Î¸â‚‚ given Î¸â‚â½áµ—âºÂ¹â¾
â”‚   â”‚   â””â”€ Still monotonic
â”‚   â”œâ”€ Expectation-Conditional Maximization Either (ECME):
â”‚   â”‚   â”œâ”€ Some CM-steps maximize observed-data likelihood
â”‚   â”‚   â””â”€ Faster convergence
â”‚   â”œâ”€ Generalized EM (GEM):
â”‚   â”‚   â”œâ”€ M-step only improves: Q(Î¸â½áµ—âºÂ¹â¾) â‰¥ Q(Î¸â½áµ—â¾)
â”‚   â”‚   â””â”€ Useful for constrained optimization
â”‚   â”œâ”€ Incremental EM:
â”‚   â”‚   â”œâ”€ Online learning (streaming data)
â”‚   â”‚   â””â”€ Update after each observation
â”‚   â””â”€ Variational EM:
â”‚       â”œâ”€ Approximate posterior q(Z) (not exact)
â”‚       â””â”€ Variational Bayes inference
â”œâ”€ Standard Errors & Inference:
â”‚   â”œâ”€ Observed Information:
â”‚   â”‚   â”œâ”€ I_obs(Î¸Ì‚) = -âˆ‚Â²log L(Î¸|Y)/âˆ‚Î¸âˆ‚Î¸'|_{Î¸Ì‚}
â”‚   â”‚   â”œâ”€ Numerical Hessian at convergence
â”‚   â”‚   â””â”€ SE(Î¸Ì‚) = âˆšdiag(I_obsâ»Â¹)
â”‚   â”œâ”€ Louis's Formula:
â”‚   â”‚   â”œâ”€ I_obs = I_complete - Var[S_complete|Y]
â”‚   â”‚   â”œâ”€ Use E-step calculations
â”‚   â”‚   â””â”€ Computationally efficient
â”‚   â”œâ”€ Bootstrap:
â”‚   â”‚   â”œâ”€ Resample data, re-run EM
â”‚   â”‚   â””â”€ SE from bootstrap distribution
â”‚   â”œâ”€ Parametric Bootstrap:
â”‚   â”‚   â”œâ”€ Simulate data from p(Â·|Î¸Ì‚)
â”‚   â”‚   â””â”€ Account for missing data structure
â”‚   â””â”€ Supplemented EM (SEM):
â”‚       â”œâ”€ Simultaneously estimate Î¸ and I_obs
â”‚       â””â”€ One-step Newton-Raphson after EM
â”œâ”€ Model Selection:
â”‚   â”œâ”€ Number of Components (K):
â”‚   â”‚   â”œâ”€ BIC: log L - (k/2)log(n) (prefer lower)
â”‚   â”‚   â”œâ”€ AIC: log L - k
â”‚   â”‚   â””â”€ Integrated classification likelihood (ICL)
â”‚   â”œâ”€ Cross-Validation:
â”‚   â”‚   â”œâ”€ K-fold CV on held-out data
â”‚   â”‚   â””â”€ Avoid overfitting
â”‚   â”œâ”€ Silhouette Score:
â”‚   â”‚   â””â”€ Cluster quality measure
â”‚   â””â”€ Likelihood Ratio Test:
â”‚       â”œâ”€ LR = 2[log L(K) - log L(K-1)]
â”‚       â””â”€ Not standard Ï‡Â² (boundary issue)
â”œâ”€ Computational Considerations:
â”‚   â”œâ”€ Initialization Sensitivity:
â”‚   â”‚   â”œâ”€ Multiple random starts (10-100)
â”‚   â”‚   â”œâ”€ K-means for mixture models
â”‚   â”‚   â””â”€ Select highest likelihood
â”‚   â”œâ”€ Convergence Diagnostics:
â”‚   â”‚   â”œâ”€ Plot log-likelihood vs iteration
â”‚   â”‚   â”œâ”€ Check parameter stability
â”‚   â”‚   â””â”€ Monitor Q-function increase
â”‚   â”œâ”€ Numerical Stability:
â”‚   â”‚   â”œâ”€ Log-sum-exp trick for probabilities
â”‚   â”‚   â”œâ”€ Regularization (add small Îµ to covariance)
â”‚   â”‚   â””â”€ Avoid underflow in responsibilities
â”‚   â””â”€ Computational Complexity:
â”‚       â”œâ”€ GMM: O(nKdÂ²) per iteration
â”‚       â”œâ”€ HMM: O(TKÂ²) (forward-backward)
â”‚       â””â”€ Typically 10-100 iterations
â””â”€ Applications:
    â”œâ”€ Mixture Models: Clustering, density estimation
    â”œâ”€ Missing Data: Multiple imputation, survey data
    â”œâ”€ Hidden Markov Models: Time series, speech, finance
    â”œâ”€ Factor Analysis: Psychometrics, dimension reduction
    â”œâ”€ Item Response Theory: Educational testing
    â”œâ”€ Survival Analysis: Interval censoring
    â”œâ”€ Image Segmentation: Computer vision
    â”œâ”€ Bioinformatics: Gene expression, motif finding
    â””â”€ Econometrics: Regime switching, censored models
```

**Interaction:** Initialize Î¸â½â°â¾ â†’ E-step: Compute Q(Î¸|Î¸â½áµ—â¾) â†’ M-step: Î¸â½áµ—âºÂ¹â¾ = argmax Q â†’ Check convergence â†’ Repeat until converged

## 5. Mini-Project
Implement EM for Gaussian mixture model with missing data:
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
from scipy.special import logsumexp
import seaborn as sns

np.random.seed(321)

# ===== Simulate Gaussian Mixture Data =====
print("="*80)
print("EXPECTATION-MAXIMIZATION (EM) ALGORITHM")
print("="*80)

n = 500
K = 3  # Number of clusters

# True parameters
pi_true = np.array([0.3, 0.5, 0.2])
mu_true = np.array([[-2, -2], [0, 3], [3, 0]])
Sigma_true = np.array([
    [[1.0, 0.3], [0.3, 1.0]],
    [[0.8, -0.2], [-0.2, 0.8]],
    [[1.2, 0.5], [0.5, 1.2]]
])

print(f"Simulation Setup:")
print(f"  Sample size: {n}")
print(f"  Number of components: {K}")
print(f"  True mixing proportions: {pi_true}")

# Generate data
cluster_labels = np.random.choice(K, size=n, p=pi_true)
Y_complete = np.zeros((n, 2))

for k in range(K):
    mask = cluster_labels == k
    n_k = np.sum(mask)
    Y_complete[mask] = np.random.multivariate_normal(
        mu_true[k], Sigma_true[k], size=n_k
    )

print(f"  True cluster sizes: {np.bincount(cluster_labels)}")

# Introduce missing data (MCAR)
missing_prob = 0.20
missing_mask = np.random.rand(n, 2) < missing_prob
Y_observed = Y_complete.copy()
Y_observed[missing_mask] = np.nan

n_missing = np.sum(missing_mask)
missing_pct = n_missing / (n * 2) * 100

print(f"\nMissing Data:")
print(f"  Total missing values: {n_missing}/{n*2} ({missing_pct:.1f}%)")
print(f"  Rows with any missing: {np.sum(np.any(missing_mask, axis=1))}/{n}")

# ===== EM Algorithm for GMM with Missing Data =====
print("\n" + "="*80)
print("EM ALGORITHM IMPLEMENTATION")
print("="*80)

def initialize_params(Y, K, method='kmeans'):
    """Initialize parameters"""
    n, d = Y.shape
    
    # Use complete cases for initialization
    complete_cases = ~np.any(np.isnan(Y), axis=1)
    Y_complete_init = Y[complete_cases]
    
    if method == 'kmeans':
        # Simple K-means on complete cases
        from sklearn.cluster import KMeans
        kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)
        labels = kmeans.fit_predict(Y_complete_init)
        
        pi = np.bincount(labels, minlength=K) / len(labels)
        mu = kmeans.cluster_centers_
        
        # Initialize covariances
        Sigma = np.zeros((K, d, d))
        for k in range(K):
            if np.sum(labels == k) > 1:
                Y_k = Y_complete_init[labels == k]
                Sigma[k] = np.cov(Y_k.T) + np.eye(d) * 0.1
            else:
                Sigma[k] = np.eye(d)
    
    elif method == 'random':
        # Random initialization
        pi = np.ones(K) / K
        idx = np.random.choice(len(Y_complete_init), K, replace=False)
        mu = Y_complete_init[idx]
        Sigma = np.array([np.eye(d) for _ in range(K)])
    
    return pi, mu, Sigma

def mvn_logpdf(Y, mu, Sigma):
    """Multivariate normal log-pdf handling missing data"""
    n, d = Y.shape
    logprob = np.zeros(n)
    
    for i in range(n):
        obs_idx = ~np.isnan(Y[i])
        
        if np.sum(obs_idx) == 0:
            logprob[i] = 0  # No data, uniform
            continue
        
        y_obs = Y[i, obs_idx]
        mu_obs = mu[obs_idx]
        Sigma_obs = Sigma[np.ix_(obs_idx, obs_idx)]
        
        # Log-pdf
        try:
            logprob[i] = stats.multivariate_normal.logpdf(
                y_obs, mu_obs, Sigma_obs
            )
        except:
            logprob[i] = -1e10  # Numerical issue
    
    return logprob

def e_step(Y, pi, mu, Sigma):
    """E-step: Compute responsibilities"""
    n = len(Y)
    K = len(pi)
    
    # Log-responsibilities (n x K)
    log_resp = np.zeros((n, K))
    
    for k in range(K):
        log_resp[:, k] = np.log(pi[k] + 1e-10) + mvn_logpdf(Y, mu[k], Sigma[k])
    
    # Normalize (log-sum-exp trick)
    log_sum = logsumexp(log_resp, axis=1, keepdims=True)
    log_resp -= log_sum
    resp = np.exp(log_resp)
    
    # Log-likelihood
    loglik = np.sum(log_sum)
    
    return resp, loglik

def impute_missing(Y, resp, mu, Sigma):
    """Impute missing values using current parameters"""
    Y_imputed = Y.copy()
    n, d = Y.shape
    K = len(mu)
    
    for i in range(n):
        if np.any(np.isnan(Y[i])):
            obs_idx = ~np.isnan(Y[i])
            miss_idx = np.isnan(Y[i])
            
            if np.sum(obs_idx) == 0:
                # No observed values: Use weighted mean
                Y_imputed[i] = np.sum(resp[i][:, None] * mu, axis=0)
            else:
                # Conditional expectation E[Y_miss|Y_obs, k]
                imputed_values = np.zeros(d)
                
                for k in range(K):
                    y_obs = Y[i, obs_idx]
                    mu_obs = mu[k, obs_idx]
                    mu_miss = mu[k, miss_idx]
                    
                    Sigma_obs_obs = Sigma[k][np.ix_(obs_idx, obs_idx)]
                    Sigma_miss_obs = Sigma[k][np.ix_(miss_idx, obs_idx)]
                    
                    try:
                        Sigma_obs_inv = np.linalg.inv(Sigma_obs_obs)
                        conditional_mean = mu_miss + Sigma_miss_obs @ Sigma_obs_inv @ (y_obs - mu_obs)
                        imputed_values[miss_idx] += resp[i, k] * conditional_mean
                    except:
                        imputed_values[miss_idx] += resp[i, k] * mu_miss
                
                Y_imputed[i, miss_idx] = imputed_values[miss_idx]
    
    return Y_imputed

def m_step(Y, resp):
    """M-step: Update parameters"""
    n, d = Y.shape
    K = resp.shape[1]
    
    # Effective sample sizes
    n_k = np.sum(resp, axis=0)
    
    # Mixing proportions
    pi = n_k / n
    
    # Means (weighted)
    mu = np.zeros((K, d))
    for k in range(K):
        mu[k] = np.sum(resp[:, k][:, None] * Y, axis=0) / n_k[k]
    
    # Covariances (weighted)
    Sigma = np.zeros((K, d, d))
    for k in range(K):
        Y_centered = Y - mu[k]
        Sigma[k] = (resp[:, k][:, None, None] * Y_centered[:, :, None] * Y_centered[:, None, :]).sum(axis=0) / n_k[k]
        
        # Regularization
        Sigma[k] += np.eye(d) * 1e-6
    
    return pi, mu, Sigma

# Initialize
pi, mu, Sigma = initialize_params(Y_observed, K, method='kmeans')

print(f"Initialization:")
print(f"  Ï€: {pi}")
print(f"  Î¼:\n{mu}")

# EM Iterations
max_iter = 100
tol = 1e-6
loglik_history = []

print(f"\nRunning EM Algorithm:")
print(f"  Max iterations: {max_iter}")
print(f"  Tolerance: {tol}")

for t in range(max_iter):
    # E-step
    resp, loglik = e_step(Y_observed, pi, mu, Sigma)
    loglik_history.append(loglik)
    
    # Impute missing data
    Y_imputed = impute_missing(Y_observed, resp, mu, Sigma)
    
    # M-step
    pi_new, mu_new, Sigma_new = m_step(Y_imputed, resp)
    
    # Check convergence
    if t > 0:
        loglik_change = loglik - loglik_history[-2]
        rel_change = abs(loglik_change) / abs(loglik_history[-2])
        
        if t % 10 == 0:
            print(f"  Iter {t:3d}: log-lik = {loglik:.2f}, "
                  f"change = {loglik_change:+.4f}")
        
        if rel_change < tol:
            print(f"\n  âœ“ Converged at iteration {t}")
            print(f"    Final log-likelihood: {loglik:.4f}")
            break
    
    pi, mu, Sigma = pi_new, mu_new, Sigma_new
else:
    print(f"\n  âš  Maximum iterations reached")

# Final responsibilities
resp_final, loglik_final = e_step(Y_observed, pi, mu, Sigma)
cluster_pred = np.argmax(resp_final, axis=1)

print(f"\nFinal Parameters:")
print(f"  Ï€: {pi}")
print(f"  Î¼:\n{mu}")

print(f"\nPredicted Cluster Sizes: {np.bincount(cluster_pred, minlength=K)}")

# ===== Model Comparison: Complete Data vs EM with Missing =====
print("\n" + "="*80)
print("COMPARISON: COMPLETE DATA vs MISSING DATA EM")
print("="*80)

# Run EM on complete data
pi_complete, mu_complete, Sigma_complete = initialize_params(Y_complete, K)

for t in range(max_iter):
    resp_complete, _ = e_step(Y_complete, pi_complete, mu_complete, Sigma_complete)
    pi_complete, mu_complete, Sigma_complete = m_step(Y_complete, resp_complete)
    
    if t > 0 and abs(loglik_history[-1] - loglik_history[-2]) < tol:
        break

print(f"Complete Data Estimates:")
print(f"  Ï€: {pi_complete}")
print(f"  Î¼:\n{mu_complete}")

print(f"\nMissing Data EM Estimates:")
print(f"  Ï€: {pi}")
print(f"  Î¼:\n{mu}")

print(f"\nTrue Parameters:")
print(f"  Ï€: {pi_true}")
print(f"  Î¼:\n{mu_true}")

# ===== Multiple Random Starts =====
print("\n" + "="*80)
print("MULTIPLE RANDOM STARTS")
print("="*80)

n_starts = 10
best_loglik = -np.inf
best_params = None

print(f"Running {n_starts} random initializations...")

for start in range(n_starts):
    np.random.seed(start)
    
    pi_init, mu_init, Sigma_init = initialize_params(Y_observed, K, method='random')
    
    pi_temp, mu_temp, Sigma_temp = pi_init, mu_init, Sigma_init
    
    for t in range(max_iter):
        resp_temp, loglik_temp = e_step(Y_observed, pi_temp, mu_temp, Sigma_temp)
        Y_imputed_temp = impute_missing(Y_observed, resp_temp, mu_temp, Sigma_temp)
        pi_temp, mu_temp, Sigma_temp = m_step(Y_imputed_temp, resp_temp)
        
        if t > 0 and abs(loglik_temp - loglik_history[-1]) < tol:
            break
    
    resp_temp, loglik_temp = e_step(Y_observed, pi_temp, mu_temp, Sigma_temp)
    
    print(f"  Start {start+1}: log-lik = {loglik_temp:.2f}")
    
    if loglik_temp > best_loglik:
        best_loglik = loglik_temp
        best_params = (pi_temp, mu_temp, Sigma_temp)

print(f"\nBest log-likelihood: {best_loglik:.4f}")
print(f"Original initialization: {loglik_final:.4f}")
print(f"Improvement: {best_loglik - loglik_final:.4f}")

# ===== Visualizations =====
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# Plot 1: True Data with True Clusters
axes[0, 0].scatter(Y_complete[:, 0], Y_complete[:, 1], 
                  c=cluster_labels, cmap='viridis', alpha=0.6, s=30)
axes[0, 0].scatter(mu_true[:, 0], mu_true[:, 1], 
                  c='red', marker='X', s=200, edgecolors='black', 
                  linewidths=2, label='True Centers')
axes[0, 0].set_xlabel('Xâ‚')
axes[0, 0].set_ylabel('Xâ‚‚')
axes[0, 0].set_title('True Data (Complete)')
axes[0, 0].legend()
axes[0, 0].grid(alpha=0.3)

# Plot 2: Observed Data (with missing)
Y_plot = Y_observed.copy()
complete_mask = ~np.any(np.isnan(Y_observed), axis=1)

axes[0, 1].scatter(Y_plot[complete_mask, 0], Y_plot[complete_mask, 1],
                  c=cluster_pred[complete_mask], cmap='viridis', 
                  alpha=0.6, s=30, label='Complete obs')
axes[0, 1].scatter(mu[:, 0], mu[:, 1], 
                  c='red', marker='X', s=200, edgecolors='black', 
                  linewidths=2, label='EM Centers')
axes[0, 1].set_xlabel('Xâ‚')
axes[0, 1].set_ylabel('Xâ‚‚')
axes[0, 1].set_title(f'EM Clustering ({missing_pct:.0f}% Missing)')
axes[0, 1].legend()
axes[0, 1].grid(alpha=0.3)

# Plot 3: Log-Likelihood Convergence
axes[0, 2].plot(loglik_history, linewidth=2)
axes[0, 2].set_xlabel('Iteration')
axes[0, 2].set_ylabel('Log-Likelihood')
axes[0, 2].set_title('EM Convergence')
axes[0, 2].grid(alpha=0.3)

# Plot 4: Responsibilities Heatmap
axes[1, 0].imshow(resp_final[:50].T, aspect='auto', cmap='YlOrRd', 
                 interpolation='nearest')
axes[1, 0].set_xlabel('Observation')
axes[1, 0].set_ylabel('Component')
axes[1, 0].set_title('Responsibilities Î³áµ¢â‚– (first 50 obs)')
axes[1, 0].set_yticks([0, 1, 2])
axes[1, 0].colorbar = plt.colorbar(
    axes[1, 0].images[0], ax=axes[1, 0], fraction=0.046
)

# Plot 5: Hard vs Soft Clustering
uncertainty = 1 + np.sum(resp_final * np.log(resp_final + 1e-10), axis=1) / np.log(K)
axes[1, 1].scatter(Y_complete[:, 0], Y_complete[:, 1], 
                  c=uncertainty, cmap='coolwarm', alpha=0.6, s=30)
axes[1, 1].scatter(mu[:, 0], mu[:, 1], 
                  c='black', marker='X', s=200, edgecolors='white', 
                  linewidths=2)
axes[1, 1].set_xlabel('Xâ‚')
axes[1, 1].set_ylabel('Xâ‚‚')
axes[1, 1].set_title('Clustering Uncertainty (Entropy)')
cbar = plt.colorbar(axes[1, 1].collections[0], ax=axes[1, 1])
cbar.set_label('Certainty')
axes[1, 1].grid(alpha=0.3)

# Plot 6: Parameter Comparison
param_names = [f'Ï€â‚', f'Ï€â‚‚', f'Ï€â‚ƒ', 
               f'Î¼â‚â‚', f'Î¼â‚â‚‚', f'Î¼â‚‚â‚', f'Î¼â‚‚â‚‚', f'Î¼â‚ƒâ‚', f'Î¼â‚ƒâ‚‚']
true_vals = np.concatenate([pi_true, mu_true.flatten()])
em_vals = np.concatenate([pi, mu.flatten()])
complete_vals = np.concatenate([pi_complete, mu_complete.flatten()])

x_pos = np.arange(len(param_names))
width = 0.25

axes[1, 2].bar(x_pos - width, true_vals, width, label='True', alpha=0.7)
axes[1, 2].bar(x_pos, em_vals, width, label='EM (Missing)', alpha=0.7)
axes[1, 2].bar(x_pos + width, complete_vals, width, 
              label='EM (Complete)', alpha=0.7)
axes[1, 2].set_xticks(x_pos)
axes[1, 2].set_xticklabels(param_names, rotation=45)
axes[1, 2].set_ylabel('Parameter Value')
axes[1, 2].set_title('Parameter Estimates Comparison')
axes[1, 2].legend(fontsize=8)
axes[1, 2].grid(alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('em_algorithm_analysis.png', dpi=150, bbox_inches='tight')
plt.show()

# ===== Summary =====
print("\n" + "="*80)
print("SUMMARY AND INSIGHTS")
print("="*80)

print("\n1. Convergence Properties:")
print(f"   Iterations to converge: {len(loglik_history)}")
print(f"   Final log-likelihood: {loglik_final:.4f}")
print(f"   Monotonic increase: âœ“ (guaranteed by EM)")

print("\n2. Missing Data Handling:")
print(f"   {missing_pct:.1f}% data missing (MCAR)")
print(f"   EM imputes via E[Y_miss|Y_obs, Î¸]")
print(f"   Accounts for uncertainty in imputation")

print("\n3. Parameter Recovery:")
mse_pi = np.mean((pi - pi_true)**2)
mse_mu = np.mean((mu.flatten() - mu_true.flatten())**2)
print(f"   MSE(Ï€): {mse_pi:.6f}")
print(f"   MSE(Î¼): {mse_mu:.6f}")

print("\n4. Multiple Initializations:")
print(f"   {n_starts} random starts explored")
print(f"   Local maxima issue mitigated")
print(f"   Best found {best_loglik - loglik_final:+.4f} better")

print("\n5. Practical Recommendations:")
print("   â€¢ Use K-means or hierarchical clustering for initialization")
print("   â€¢ Run multiple random starts (10-100)")
print("   â€¢ Monitor log-likelihood for convergence")
print("   â€¢ Check responsibilities for cluster uncertainty")
print("   â€¢ Regularize covariances (add small Îµ to diagonal)")
print("   â€¢ Use BIC/AIC for selecting K")

print("\n6. EM Advantages:")
print("   â€¢ Handles missing data naturally (MAR assumption)")
print("   â€¢ Guaranteed monotonic likelihood increase")
print("   â€¢ Often closed-form M-step")
print("   â€¢ Interpretable latent structure")
print("   â€¢ Flexible framework (HMM, factor analysis, etc.)")

print("\n7. Limitations:")
print("   âš  Converges to local maximum (initialization critical)")
print("   âš  Slow convergence with high missing data")
print("   âš  Identifiability issues (label switching)")
print("   âš  Requires correct model specification")
print("   âš  Standard errors require Louis's formula or bootstrap")
```

## 6. Challenge Round
When does EM fail or mislead?
- **Local maxima**: Non-convex likelihood â†’ Solution depends on initialization; multiple random starts required; GMM with K>2 highly multimodal
- **Identifiability**: Mixture labels arbitrary (permutation invariance) â†’ Post-hoc matching; label switching across runs
- **Slow convergence**: High fraction missing information â†’ Linear rate Î»=(fraction missing); accelerated EM or quasi-Newton methods
- **Model misspecification**: Wrong number of components K â†’ BIC/AIC for selection; overfitting if K too large
- **Singular covariances**: Cluster collapses to single point â†’ Regularize Î£Ì‚â‚– + ÎµI; constrain minimum eigenvalue
- **Not Missing at Random (NMAR)**: Missingness depends on unobserved values â†’ EM biased; need selection model or sensitivity analysis

## 7. Key References
- [Dempster, Laird & Rubin (1977) - Maximum Likelihood from Incomplete Data via the EM Algorithm](https://www.jstor.org/stable/2984875)
- [McLachlan & Krishnan (2008) - The EM Algorithm and Extensions](https://onlinelibrary.wiley.com/doi/book/10.1002/9780470191613)
- [Bishop (2006) - Pattern Recognition and Machine Learning, Chapter 9](https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/)

---
**Status:** Foundational for latent variable models | **Complements:** MLE, Missing Data Imputation, Mixture Models, HMM
